import OpenAI from 'openai';
import { Persona } from '@/types';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY || '',
  baseURL: process.env.OPENAI_BASE_URL || 'https://api.openai.com/v1',
});

export interface LLMResponse {
  content: string;
  emotion?: string;
}

export class LLMService {
  private async callOpenAI(persona: Persona, message: string): Promise<LLMResponse> {
    try {
      const response = await openai.chat.completions.create({
        model: process.env.OPENAI_MODEL || 'gpt-3.5-turbo',
        messages: [
          {
            role: 'system',
            content: persona.systemPrompt
          },
          {
            role: 'user',
            content: message
          }
        ],
        stream: true,
        max_tokens: 500,
      });

      let fullContent = '';
      for await (const chunk of response) {
        const content = chunk.choices[0]?.delta?.content || '';
        fullContent += content;
      }

      return { content: fullContent };
    } catch (error) {
      console.error('OpenAI API Error:', error);
      throw new Error('Failed to get response from OpenAI');
    }
  }

  private async callLocalLLM(persona: Persona, message: string): Promise<LLMResponse> {
    // æœ¬åœ°LLMå®ç°ï¼ˆå¯æ›¿æ¢ä¸ºå®é™…æœ¬åœ°æ¨¡å‹è°ƒç”¨ï¼‰
    const response = await fetch(process.env.LOCAL_LLM_URL || 'http://localhost:8080/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: process.env.LOCAL_LLM_MODEL || 'local-model',
        messages: [
          { role: 'system', content: persona.systemPrompt },
          { role: 'user', content: message }
        ],
        stream: true,
        max_tokens: 500,
      }),
    });

    if (!response.ok) {
      throw new Error(`Local LLM API error: ${response.status}`);
    }

    const reader = response.body?.getReader();
    const decoder = new TextDecoder();
    let fullContent = '';

    if (reader) {
      try {
        while (true) {
          const { done, value } = await reader.read();
          if (done) break;
          
          const chunk = decoder.decode(value);
          const lines = chunk.split('\n');
          
          for (const line of lines) {
            if (line.startsWith('data: ') && !line.includes('[DONE]')) {
              const data = JSON.parse(line.slice(6));
              const content = data.choices[0]?.delta?.content || '';
              fullContent += content;
            }
          }
        }
      } finally {
        reader.releaseLock();
      }
    }

    return { content: fullContent };
  }

  async getResponse(persona: Persona, message: string): Promise<LLMResponse> {
    if (process.env.OPENAI_API_KEY) {
      return this.callOpenAI(persona, message);
    } else if (process.env.LOCAL_LLM_URL) {
      return this.callLocalLLM(persona, message);
    } else {
      // æ¨¡æ‹Ÿå“åº”ï¼ˆç”¨äºå¼€å‘æµ‹è¯•ï¼‰
      return this.getMockResponse(persona, message);
    }
  }

  private async getMockResponse(persona: Persona, message: string): Promise<LLMResponse> {
    // æ¨¡æ‹Ÿä¸åŒäººæ ¼çš„å“åº”
    const responses: Record<string, string> = {
      gentle: 'ä½ å¥½ï¼å¾ˆé«˜å…´å’Œä½ èŠå¤©ã€‚ä»Šå¤©è¿‡å¾—æ€ä¹ˆæ ·ï¼ŸğŸ˜Š',
      sassy: 'å“¦ï¼Ÿåˆæ¥ä¸€ä¸ªèŠå¤©çš„ã€‚è¯´å§ï¼Œæˆ‘å¬ç€å‘¢ã€‚ğŸ˜',
      academic: 'æ‚¨å¥½ã€‚è¿™æ˜¯ä¸€ä¸ªå­¦æœ¯æ€§çš„å¯¹è¯å¼€å§‹ã€‚è¯·é—®æ‚¨æœ‰ä»€ä¹ˆé—®é¢˜éœ€è¦æ¢è®¨ï¼ŸğŸ§ ',
      healing: 'å—¨ï½æ¬¢è¿æ¥åˆ°è¿™é‡Œï¼å¸Œæœ›ä½ ä»Šå¤©å¿ƒæƒ…å¾ˆå¥½å‘¢ï½ ğŸ’–'
    };

    await new Promise(resolve => setTimeout(resolve, 1000));
    
    return {
      content: responses[persona.id] || 'ä½ å¥½ï¼å¾ˆé«˜å…´ä¸ºä½ æœåŠ¡ã€‚',
      emotion: 'happy'
    };
  }
}

export const llmService = new LLMService();